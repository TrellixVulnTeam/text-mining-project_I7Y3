{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from rouge import Rouge\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizerFast\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "import bleurt.score as bleurt_score\n",
    "import jieba\n",
    "import re\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "# model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#LightGBM\n",
    "import lightgbm as lgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, ensemble\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split,KFold\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metric_from_groupSize = lambda n: ('rouge-%d' % n)\n",
    "\n",
    "def calculate_recall(ref, candidate, group_size=1):\n",
    "    #print(ref)\n",
    "    try: \n",
    "        #print(candidate)\n",
    "        #Instantiate Rouge\n",
    "        rouge_metric = get_metric_from_groupSize(group_size)\n",
    "        rouge_stat = \"r\"\n",
    "        rouge = Rouge(metrics=[rouge_metric], stats=[rouge_stat])\n",
    "        outcome = rouge.get_scores(candidate, ref)\n",
    "        return outcome[0][rouge_metric][rouge_stat]\n",
    "    except:\n",
    "        return None\n",
    "          \n",
    "def calculate_precision(ref, candidate, group_size=1):\n",
    "    try:\n",
    "        #Instantiate Rouge\n",
    "        rouge_metric = get_metric_from_groupSize(group_size)\n",
    "        rouge_stat = \"p\"\n",
    "        rouge = Rouge(metrics=[rouge_metric], stats=[rouge_stat])\n",
    "\n",
    "        outcome = rouge.get_scores(candidate, ref)\n",
    "        return outcome[0][rouge_metric][rouge_stat]\n",
    "    except:\n",
    "        return None    \n",
    "def calculate_f1(ref, candidate, group_size=1):\n",
    "    try:\n",
    "        #Instantiate Rouge\n",
    "        rouge_metric = get_metric_from_groupSize(group_size)\n",
    "        rouge_stat = \"f\"\n",
    "        rouge = Rouge(metrics=[rouge_metric], stats=[rouge_stat])\n",
    "\n",
    "        outcome = rouge.get_scores(candidate, ref)\n",
    "        return outcome[0][rouge_metric][rouge_stat]\n",
    "    except:\n",
    "        return None    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BLEU(ref, candidate, group_size): \n",
    "    ref = str(ref).split()\n",
    "    candidate = str(candidate).split()\n",
    "    if(group_size > 1):\n",
    "        ref = formGroups(ref, group_size)\n",
    "        candidate = formGroups(candidate, group_size)\n",
    "\n",
    "    # compute word frequencies for the references and the candidate\n",
    "    ref_counts = Counter(ref)\n",
    "    candidate_counts = Counter(candidate)\n",
    "\n",
    "    covered = 0\n",
    "    total = 0\n",
    "    \n",
    "    # compute the coverage for each word\n",
    "    for word, count in candidate_counts.items():\n",
    "        covered += min(count, ref_counts[word])\n",
    "        total += count\n",
    "    \n",
    "    return covered / len(candidate_counts)\n",
    "\n",
    "def formGroups(words, group_size):\n",
    "\n",
    "    if(group_size > len(words)):\n",
    "        return words\n",
    "\n",
    "    #Form groups\n",
    "    to_return = []\n",
    "    i = 0\n",
    "    while (i + group_size - 1) < len(words):\n",
    "        to_return.append(' '.join(words[i:i + group_size]))\n",
    "        i+=1\n",
    "\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractWordsIntoList(sentence):\n",
    "    return jieba.lcut(sentence)\n",
    "\n",
    "def extractWordsAndSeparateByCharacter(sentence, separator=\" \"):\n",
    "    return separator.join(extractWordsIntoList(sentence))\n",
    "\n",
    "def test():\n",
    "    randomStrings = [\"小明硕士毕业于中国科学院计算所\",\"我来到北京清华大学\",\"小明硕士毕业于中国科学院计算所，后在日本京都大学深造\"]\n",
    "    \n",
    "    for string in randomStrings:\n",
    "        output = extractWordsAndSeparateByCharacter(string)\n",
    "        print(\"\\nBefore: %s\" % string)\n",
    "        print(\"After: %s\\n\" % output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BLEU\n",
    "def calculate_bleu(dataframe, reference_col='reference', translation_col='translation', word_group_size=1):\n",
    "    series_data = [BLEU(row[reference_col], row[translation_col], group_size= word_group_size) for idx, row in dataframe.iterrows()]\n",
    "    return pd.Series(data=series_data, index=dataframe.index) \n",
    "\n",
    "#ROUGE\n",
    "def calculate_rouge_recall(dataframe, reference_col='reference', translation_col='translation', word_group_size=1):\n",
    "    series_data = [calculate_recall(row[reference_col], row[translation_col], word_group_size) for idx, row in dataframe.iterrows()]\n",
    "    return pd.Series(data=series_data, index=dataframe.index) \n",
    "\n",
    "def calculate_rouge_precision(dataframe, reference_col='reference', translation_col='translation', word_group_size=1):\n",
    "    series_data = [calculate_precision(row[reference_col], row[translation_col], word_group_size) for idx, row in dataframe.iterrows()]\n",
    "    return pd.Series(data=series_data, index=dataframe.index) \n",
    "\n",
    "def calculate_rouge_f1(dataframe, reference_col='reference', translation_col='translation', word_group_size=1):\n",
    "    series_data = [calculate_f1(row[reference_col], row[translation_col], word_group_size) for idx, row in dataframe.iterrows()]\n",
    "    return pd.Series(data=series_data, index=dataframe.index) \n",
    "\n",
    "#AUXILIARY\n",
    "def getCSV(path):\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def non_text_removal(text):\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text).lower()\n",
    "    if not text.strip():\n",
    "        text = \"Non text\"\n",
    "    #text = remove_stopwords(text)\n",
    "    return text\n",
    "\n",
    "def preProcessDataframe(dataframe, filePath, reference_col='reference', translation_col='translation'):\n",
    "    if 'en-zh' in filePath:\n",
    "        print(\"Detected en-zh... Pre-Processing Chinese...\")\n",
    "        series_data = [extractWordsAndSeparateByCharacter(row[translation_col]) for idx, row in dataframe.iterrows()]\n",
    "        dataframe[translation_col] = pd.Series(data=series_data, index=dataframe.index)\n",
    "        series_data = [extractWordsAndSeparateByCharacter(row[reference_col]) for idx, row in dataframe.iterrows()]\n",
    "        dataframe[reference_col] = pd.Series(data=series_data, index=dataframe.index)\n",
    "    elif \"ru-en\" in filePath:\n",
    "        print(\"Detected ru-en...Skip preprocessing...\")\n",
    "        # only convert non-string to string\n",
    "        dataframe['reference'] =  pd.Series([str(row[1]) for row in dataframe['reference'].iteritems()])\n",
    "        dataframe['translation'] =  pd.Series([str(row[1]) for row in dataframe['translation'].iteritems()])   \n",
    "        # no preprocessing for these pairs as the score decrease\n",
    "        return dataframe     \n",
    "    elif \"en-fi\" in filePath:\n",
    "        print(\"Detected ru-en...Skip preprocessing...\")\n",
    "        # only convert non-string to string\n",
    "        dataframe['reference'] =  pd.Series([str(row[1]) for row in dataframe['reference'].iteritems()])\n",
    "        dataframe['translation'] =  pd.Series([str(row[1]) for row in dataframe['translation'].iteritems()])\n",
    "        # no preprocessing for these pairs as the score decrease\n",
    "        return dataframe    \n",
    "    else:\n",
    "        print(\"Preprocessing...\")\n",
    "        dataframe['reference'] =  pd.Series([non_text_removal(row[1]) for row in dataframe['reference'].iteritems()])\n",
    "        dataframe['translation'] =  pd.Series([non_text_removal(row[1]) for row in dataframe['translation'].iteritems()])   \n",
    "    return dataframe\n",
    "\n",
    "def initBLEURT(csv):\n",
    "    checkpoint = \"bleurt\\\\test_checkpoint\"\n",
    "    scorer = bleurt_score.BleurtScorer(checkpoint)\n",
    "    references = csv.reference\n",
    "    candidates = csv.translation\n",
    "    print(\"BLEURT calculating...\")\n",
    "    scores_bluert = [scorer.score(references=[row[\"reference\"]],candidates= [row[\"translation\"]])[0] for idx, row in csv.iterrows()]\n",
    "    return scores_bluert\n",
    "\n",
    "#MAIN\n",
    "def main(filePath):\n",
    "    #Get file path from arguments\n",
    "    #filePath = sys.argv[1]\n",
    "    \n",
    "    #Import CSV\n",
    "    csv = getCSV(filePath)\n",
    "    csv = preProcessDataframe(csv, filePath)\n",
    "\n",
    "    #BLEU calculation\n",
    "    csv['bleu_w1'] = calculate_bleu(csv)\n",
    "    csv['bleu_w2'] = calculate_bleu(csv, word_group_size=2)\n",
    "    print(\"BLEU calculated...\")\n",
    "\n",
    "    #ROUGE calculation\n",
    "    csv['rouge_recall_w1'] = calculate_rouge_recall(csv)\n",
    "    csv['rouge_precision_w1'] = calculate_rouge_precision(csv)\n",
    "    csv['rouge_f1_w1'] = calculate_rouge_f1(csv)\n",
    "    print(\"ROUGE 1-gram calculated...\")\n",
    "\n",
    "    csv['rouge_precision_w2'] = calculate_rouge_precision(csv, word_group_size=2)\n",
    "    csv['rouge_recall_w2'] = calculate_rouge_recall(csv, word_group_size=2)\n",
    "    csv['rouge_f1_w2'] = calculate_rouge_f1(csv, word_group_size=2)\n",
    "    print(\"ROUGE 2-gram calculated...\")   \n",
    "    csv['BLEURT'] = initBLEURT(csv)\n",
    "\n",
    "    #PRINT\n",
    "    #print(csv)\n",
    "    return csv\n",
    "\n",
    "def final_results(scores,z_score):\n",
    "    mse = np.array([])\n",
    "    pcorr = np.array([])\n",
    "    kendallcorr = np.array([])\n",
    "    metrics = scores.columns\n",
    "    for col in scores.columns:\n",
    "        mse = np.append(mse,mean_squared_error(z_score,scores[col]))\n",
    "        pcorr = np.append(pcorr,z_score.corr(scores[col],method='pearson'))\n",
    "        kendallcorr = np.append(kendallcorr,z_score.corr(scores[col],method='kendall'))\n",
    "    return pd.DataFrame({'mse':mse,'pcorr':pcorr,'kendallcorr':kendallcorr},index = metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"corpus\"\n",
    "corpus = \"\\\\en-fi\"\n",
    "filePath = path + corpus + \"\\\\scores.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected ru-en...Skip preprocessing...\n",
      "BLEU calculated...\n",
      "ROUGE 1-gram calculated...\n",
      "ROUGE 2-gram calculated...\n",
      "INFO:tensorflow:Reading checkpoint bleurt\\test_checkpoint.\n",
      "INFO:tensorflow:Config file found, reading.\n",
      "INFO:tensorflow:Will load checkpoint dbleurt_tiny\n",
      "INFO:tensorflow:Loads full paths and checks that files exists.\n",
      "INFO:tensorflow:... name:dbleurt_tiny\n",
      "INFO:tensorflow:... vocab_file:vocab.txt\n",
      "INFO:tensorflow:... bert_config_file:bert_config.json\n",
      "INFO:tensorflow:... do_lower_case:True\n",
      "INFO:tensorflow:... max_seq_length:512\n",
      "INFO:tensorflow:Creating BLEURT scorer.\n",
      "INFO:tensorflow:Creating WordPiece tokenizer.\n",
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n",
      "INFO:tensorflow:Creating Eager Mode predictor.\n",
      "INFO:tensorflow:Loading model.\n",
      "INFO:tensorflow:BLEURT initialized.\n",
      "BLEURT calculating...\n"
     ]
    }
   ],
   "source": [
    "data = main(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>reference</th>\n",
       "      <th>translation</th>\n",
       "      <th>z-score</th>\n",
       "      <th>avg-score</th>\n",
       "      <th>annotators</th>\n",
       "      <th>bleu_w1</th>\n",
       "      <th>bleu_w2</th>\n",
       "      <th>rouge_recall_w1</th>\n",
       "      <th>rouge_precision_w1</th>\n",
       "      <th>rouge_f1_w1</th>\n",
       "      <th>rouge_precision_w2</th>\n",
       "      <th>rouge_recall_w2</th>\n",
       "      <th>rouge_f1_w2</th>\n",
       "      <th>BLEURT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You can turn yourself into a pineapple, a dog ...</td>\n",
       "      <td>Voit muuttaa itsesi ananasta, koirasta tai Roy...</td>\n",
       "      <td>Voit muuttaa itsesi ananakseksi, koiraksi tai ...</td>\n",
       "      <td>-0.286195</td>\n",
       "      <td>34.20</td>\n",
       "      <td>5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.316181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Also shot were three men: two 29-year-olds and...</td>\n",
       "      <td>Myös ammuttiin kolme miestä: kaksi 29-vuotiait...</td>\n",
       "      <td>Myös kolmea miestä ammuttiin: kahta 29-vuotias...</td>\n",
       "      <td>0.547076</td>\n",
       "      <td>58.40</td>\n",
       "      <td>5</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.431411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The information is stored at the cash register...</td>\n",
       "      <td>Tiedot tallennetaan kassakoneisiin joka tapauk...</td>\n",
       "      <td>Tiedot kuitenkin tallentuvat kassoilla joka ta...</td>\n",
       "      <td>1.122476</td>\n",
       "      <td>74.60</td>\n",
       "      <td>5</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.244749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Xinhua says that there were traces of hydrochl...</td>\n",
       "      <td>Xinhua kertoo, että Xinyin näytteestä oli sunn...</td>\n",
       "      <td>Xinhua kertoo, että Xinyin sunnuntaina antamas...</td>\n",
       "      <td>0.383095</td>\n",
       "      <td>53.60</td>\n",
       "      <td>5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.465652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MacDonald, who was brought on board CBC's comm...</td>\n",
       "      <td>Voitaisiin kuulla CBD: n kommenttitiimin toimi...</td>\n",
       "      <td>MacDonaldin, joka tuli CBC:n selostajatiimiin ...</td>\n",
       "      <td>-0.493065</td>\n",
       "      <td>32.25</td>\n",
       "      <td>4</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.193548</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>-0.088476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0  You can turn yourself into a pineapple, a dog ...   \n",
       "1  Also shot were three men: two 29-year-olds and...   \n",
       "2  The information is stored at the cash register...   \n",
       "3  Xinhua says that there were traces of hydrochl...   \n",
       "4  MacDonald, who was brought on board CBC's comm...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  Voit muuttaa itsesi ananasta, koirasta tai Roy...   \n",
       "1  Myös ammuttiin kolme miestä: kaksi 29-vuotiait...   \n",
       "2  Tiedot tallennetaan kassakoneisiin joka tapauk...   \n",
       "3  Xinhua kertoo, että Xinyin näytteestä oli sunn...   \n",
       "4  Voitaisiin kuulla CBD: n kommenttitiimin toimi...   \n",
       "\n",
       "                                         translation   z-score  avg-score  \\\n",
       "0  Voit muuttaa itsesi ananakseksi, koiraksi tai ... -0.286195      34.20   \n",
       "1  Myös kolmea miestä ammuttiin: kahta 29-vuotias...  0.547076      58.40   \n",
       "2  Tiedot kuitenkin tallentuvat kassoilla joka ta...  1.122476      74.60   \n",
       "3  Xinhua kertoo, että Xinyin sunnuntaina antamas...  0.383095      53.60   \n",
       "4  MacDonaldin, joka tuli CBC:n selostajatiimiin ... -0.493065      32.25   \n",
       "\n",
       "   annotators   bleu_w1   bleu_w2  rouge_recall_w1  rouge_precision_w1  \\\n",
       "0           5  0.500000  0.272727         0.545455            0.500000   \n",
       "1           5  0.222222  0.000000         0.222222            0.222222   \n",
       "2           5  0.400000  0.111111         0.444444            0.400000   \n",
       "3           5  0.500000  0.272727         0.750000            0.500000   \n",
       "4           4  0.200000  0.071429         0.187500            0.200000   \n",
       "\n",
       "   rouge_f1_w1  rouge_precision_w2  rouge_recall_w2  rouge_f1_w2    BLEURT  \n",
       "0     0.521739            0.272727         0.300000     0.285714  0.316181  \n",
       "1     0.222222            0.000000         0.000000     0.000000  0.431411  \n",
       "2     0.421053            0.111111         0.125000     0.117647  0.244749  \n",
       "3     0.600000            0.272727         0.428571     0.333333  0.465652  \n",
       "4     0.193548            0.071429         0.066667     0.068966 -0.088476  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation the metrics score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scores = data.loc[:,'bleu_w1':]\n",
    "scores = pd.DataFrame(scaler.fit_transform(scores),columns=scores.columns, index = scores.index)\n",
    "scores.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mse</th>\n",
       "      <th>pcorr</th>\n",
       "      <th>kendallcorr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bleu_w1</th>\n",
       "      <td>0.898179</td>\n",
       "      <td>0.512997</td>\n",
       "      <td>0.341524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bleu_w2</th>\n",
       "      <td>1.065216</td>\n",
       "      <td>0.418989</td>\n",
       "      <td>0.293123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge_recall_w1</th>\n",
       "      <td>0.925880</td>\n",
       "      <td>0.497407</td>\n",
       "      <td>0.329099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge_precision_w1</th>\n",
       "      <td>0.881087</td>\n",
       "      <td>0.522616</td>\n",
       "      <td>0.348401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge_f1_w1</th>\n",
       "      <td>0.888932</td>\n",
       "      <td>0.518201</td>\n",
       "      <td>0.341341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge_precision_w2</th>\n",
       "      <td>1.055880</td>\n",
       "      <td>0.424243</td>\n",
       "      <td>0.296875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge_recall_w2</th>\n",
       "      <td>1.081815</td>\n",
       "      <td>0.409647</td>\n",
       "      <td>0.288378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge_f1_w2</th>\n",
       "      <td>1.062589</td>\n",
       "      <td>0.420468</td>\n",
       "      <td>0.292348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BLEURT</th>\n",
       "      <td>0.870491</td>\n",
       "      <td>0.528580</td>\n",
       "      <td>0.335507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         mse     pcorr  kendallcorr\n",
       "bleu_w1             0.898179  0.512997     0.341524\n",
       "bleu_w2             1.065216  0.418989     0.293123\n",
       "rouge_recall_w1     0.925880  0.497407     0.329099\n",
       "rouge_precision_w1  0.881087  0.522616     0.348401\n",
       "rouge_f1_w1         0.888932  0.518201     0.341341\n",
       "rouge_precision_w2  1.055880  0.424243     0.296875\n",
       "rouge_recall_w2     1.081815  0.409647     0.288378\n",
       "rouge_f1_w2         1.062589  0.420468     0.292348\n",
       "BLEURT              0.870491  0.528580     0.335507"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results(scores,data['z-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScoreMetrics(corpus,corr_dict,scores_dict):\n",
    "    print(\"Processing\" + corpus)\n",
    "    path = \"corpus\\\\\"\n",
    "    filePath = path + corpus + \"\\\\scores.csv\"    \n",
    "    data = main(filePath)\n",
    "    # safe the score to csv file\n",
    "    #data.to_csv(\"scores\\\\\" + corpus + \".csv\")\n",
    "    scaler = StandardScaler()\n",
    "    scores = data.loc[:,'bleu_w1':]\n",
    "    scores = pd.DataFrame(scaler.fit_transform(scores),columns=scores.columns, index = scores.index)\n",
    "    scores.fillna(0,inplace=True)\n",
    "    corr_dict[corpus] = final_results(scores,data['z-score'])\n",
    "    scores_dict[corpus]=final_results(scores,data['z-score'])\n",
    "    return corr_dict, scores_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU calculated...\n",
      "ROUGE 1-gram calculated...\n",
      "ROUGE 2-gram calculated...\n",
      "BLEU calculated...\n",
      "ROUGE 1-gram calculated...\n",
      "ROUGE 2-gram calculated...\n",
      "BLEU calculated...\n",
      "ROUGE 1-gram calculated...\n",
      "ROUGE 2-gram calculated...\n",
      "BLEU calculated...\n",
      "ROUGE 1-gram calculated...\n",
      "ROUGE 2-gram calculated...\n",
      "BLEU calculated...\n",
      "ROUGE 1-gram calculated...\n",
      "ROUGE 2-gram calculated...\n",
      "BLEU calculated...\n",
      "ROUGE 1-gram calculated...\n",
      "ROUGE 2-gram calculated...\n"
     ]
    }
   ],
   "source": [
    "corpus_list = ['cs-en','de-en','en-fi','en-zh','ru-en','zh-en']\n",
    "scores_dict = {}\n",
    "corr_dict = {}\n",
    "for corpus in corpus_list:\n",
    "    corr_dict, scores_dict = getScoreMetrics(corpus,corr_dict,scores_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "corpus = \"en-fi\"\n",
    "filePath = \"scores\\\\\" + corpus  +\".csv\"\n",
    "data = pd.read_csv(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scores = data.loc[:,'bleu_w1':]\n",
    "scores = pd.DataFrame(scaler.fit_transform(scores),columns=scores.columns, index = scores.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mse</th>\n",
       "      <th>pcorr</th>\n",
       "      <th>kendallcorr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bleu_w1</th>\n",
       "      <td>0.861219</td>\n",
       "      <td>0.533798</td>\n",
       "      <td>0.354543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bleu_w2</th>\n",
       "      <td>0.992208</td>\n",
       "      <td>0.460078</td>\n",
       "      <td>0.316021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge_recall_w1</th>\n",
       "      <td>0.913888</td>\n",
       "      <td>0.504156</td>\n",
       "      <td>0.328229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge_precision_w1</th>\n",
       "      <td>0.852120</td>\n",
       "      <td>0.538919</td>\n",
       "      <td>0.357439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge_f1_w1</th>\n",
       "      <td>0.861046</td>\n",
       "      <td>0.533895</td>\n",
       "      <td>0.348998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge_precision_w2</th>\n",
       "      <td>1.006834</td>\n",
       "      <td>0.451846</td>\n",
       "      <td>0.310283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge_recall_w2</th>\n",
       "      <td>1.045857</td>\n",
       "      <td>0.429884</td>\n",
       "      <td>0.295437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge_f1_w2</th>\n",
       "      <td>1.016713</td>\n",
       "      <td>0.446286</td>\n",
       "      <td>0.303983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BLEURT</th>\n",
       "      <td>0.902396</td>\n",
       "      <td>0.510624</td>\n",
       "      <td>0.327299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         mse     pcorr  kendallcorr\n",
       "bleu_w1             0.861219  0.533798     0.354543\n",
       "bleu_w2             0.992208  0.460078     0.316021\n",
       "rouge_recall_w1     0.913888  0.504156     0.328229\n",
       "rouge_precision_w1  0.852120  0.538919     0.357439\n",
       "rouge_f1_w1         0.861046  0.533895     0.348998\n",
       "rouge_precision_w2  1.006834  0.451846     0.310283\n",
       "rouge_recall_w2     1.045857  0.429884     0.295437\n",
       "rouge_f1_w2         1.016713  0.446286     0.303983\n",
       "BLEURT              0.902396  0.510624     0.327299"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results(scores,data['z-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_CV_pipe(x_train, y_train, model, n_splits = 5):\n",
    "    all_scores = []\n",
    "    # set up k-fold\n",
    "    kf = KFold(n_splits=5)\n",
    "    oof_prediction = np.zeros((len(x_train),))\n",
    "    test_preds = 0\n",
    "    for  _, (trn_idx, val_idx) in tqdm(enumerate(kf.split(x_train, y_train))):\n",
    "        # set up the splitted data\n",
    "        train       , val        = x_train.loc[trn_idx,:], x_train.loc[val_idx,:]\n",
    "        train_target, val_target = y_train[trn_idx], y_train[val_idx]      \n",
    "        #print(train,val)\n",
    "        # encode     \n",
    "        # model fitting\n",
    "        model.fit(train, train_target)\n",
    "        # get predicted values for oof data and whole test set\n",
    "        temp_oof = model.predict(val)\n",
    "        # get predicted values for whole data set aggregate from each fold iter\n",
    "        oof_prediction[val_idx] = temp_oof    \n",
    "        fold_score = pd.Series(val_target).corr(pd.Series(temp_oof,index = val_idx),method='pearson')\n",
    "        print(fold_score)\n",
    "        all_scores.append(fold_score)               \n",
    "    return  oof_prediction, all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'n_estimators': 120,\n",
    "          'max_depth': 6,\n",
    "          'min_samples_split': 20,\n",
    "          'learning_rate': 0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.dropna(inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "scores.reset_index(drop=True,inplace=True)\n",
    "data.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-191-a31d82d47031>:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for  _, (trn_idx, val_idx) in tqdm(enumerate(kf.split(x_train, y_train))):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "624c7647320b476bae47812f14bc4af7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38131197113153414\n",
      "0.42251975962889443\n",
      "0.4348521002403214\n",
      "0.46239969926539337\n",
      "0.414271929193322\n",
      "OOF correlation score\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4323567327869856"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ensemble.GradientBoostingRegressor(**params)\n",
    "oof_prediction, all_scores = kfold_CV_pipe(scores,data['z-score'],model)\n",
    "print(\"OOF correlation score\")\n",
    "data['z-score'].corr(pd.Series(oof_prediction,index = data.index),method='pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Results from Grid Search \n",
      "\n",
      " The best estimator across ALL searched params:\n",
      " GradientBoostingRegressor(learning_rate=0.01, max_depth=4, n_estimators=300,\n",
      "                          random_state=2, subsample=0.8)\n",
      "\n",
      " The best score across ALL searched params:\n",
      " 0.33710772103374964\n",
      "\n",
      " The best parameters across ALL searched params:\n",
      " {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 300, 'subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "grid_search_params ={'learning_rate': [0.01,0.03,0.05,0.09],\n",
    "                      'subsample'    : [0.9, 0.8,0.7],\n",
    "                      'n_estimators' : [100,200,300,800],\n",
    "                      'max_depth'    : [4,6,8,10],\n",
    "                     \n",
    "                     }\n",
    "X = scores; y = data['z-score']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n",
    "GBR = ensemble.GradientBoostingRegressor(random_state=2)\n",
    "grid_GBR = GridSearchCV(estimator=GBR, param_grid = grid_search_params, cv = 3, n_jobs=-1)\n",
    "grid_GBR.fit(X_train, y_train)\n",
    "#Now we are using print statements to print the results. It will give the values of hyperparameters as a result.\n",
    "\n",
    "print(\" Results from Grid Search \" )\n",
    "print(\"\\n The best estimator across ALL searched params:\\n\",grid_GBR.best_estimator_)\n",
    "print(\"\\n The best score across ALL searched params:\\n\",grid_GBR.best_score_)\n",
    "print(\"\\n The best parameters across ALL searched params:\\n\",grid_GBR.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-175-a31d82d47031>:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for  _, (trn_idx, val_idx) in tqdm(enumerate(kf.split(x_train, y_train))):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bfd73c58e5146c1864a1c476abef789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38064915507518426\n",
      "0.4230069849581893\n",
      "0.7018342029347516\n",
      "0.5301830749654544\n",
      "0.4307576208205121\n",
      "OOF correlation score\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5246008306730199"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = grid_GBR.best_params_\n",
    "model = ensemble.GradientBoostingRegressor(**best_params)\n",
    "oof_prediction, all_scores = kfold_CV_pipe(scores,data['z-score'],model)\n",
    "print(\"OOF correlation score\")\n",
    "data['z-score'].corr(pd.Series(oof_prediction,index = data.index),method='pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Results from Grid Search \n",
      "\n",
      " The best estimator across ALL searched params:\n",
      " LGBMRegressor(learning_rate=0.03, max_depth=3, metric='rmse', n_estimators=300,\n",
      "              random_state=2, subsample=0.9)\n",
      "\n",
      " The best score across ALL searched params:\n",
      " 0.34283245841679566\n",
      "\n",
      " The best parameters across ALL searched params:\n",
      " {'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 300, 'subsample': 0.9}\n"
     ]
    }
   ],
   "source": [
    "grid_search_params ={'learning_rate': [0.005,0.01,0.03,0.05,0.09],\n",
    "                      'subsample'    : [0.9, 0.8,0.7],\n",
    "                      'n_estimators' : [100,200,300,800,1200],\n",
    "                      'max_depth'    : [3,4,5,6,8], \n",
    "                     }\n",
    "X = scores; y = data['z-score']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n",
    "LightGBM = lgb.LGBMRegressor(boosting_type='gbdt', metric= 'rmse', random_state=2)\n",
    "grid_GBR = GridSearchCV(estimator=LightGBM, param_grid = grid_search_params, cv = 3, n_jobs=-1)\n",
    "grid_GBR.fit(X_train, y_train)\n",
    "#Now we are using print statements to print the results. It will give the values of hyperparameters as a result.\n",
    "\n",
    "print(\" Results from Grid Search \" )\n",
    "print(\"\\n The best estimator across ALL searched params:\\n\",grid_GBR.best_estimator_)\n",
    "print(\"\\n The best score across ALL searched params:\\n\",grid_GBR.best_score_)\n",
    "print(\"\\n The best parameters across ALL searched params:\\n\",grid_GBR.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-a31d82d47031>:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for  _, (trn_idx, val_idx) in tqdm(enumerate(kf.split(x_train, y_train))):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35bf683cb57f4f73a60e0043d2120eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39188266306002856\n",
      "0.4201159817793041\n",
      "0.7157482094631457\n",
      "0.5393351632656764\n",
      "0.4303508632519454\n",
      "OOF correlation score\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5374586649686021"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = grid_GBR.best_params_\n",
    "model = ensemble.GradientBoostingRegressor(**best_params)\n",
    "oof_prediction, all_scores = kfold_CV_pipe(scores,data['z-score'],model)\n",
    "print(\"OOF correlation score\")\n",
    "data['z-score'].corr(pd.Series(oof_prediction,index = data.index),method='pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is for saving the best model for each language pairs:\n",
    "- cs-en: \n",
    "    - GBR {'learning_rate': 0.03, 'max_depth': 4, 'n_estimators': 200, 'subsample': 0.8} -> 0.526041\n",
    "    - Best PCorr - BLEURT -> 0.465266\n",
    "- de-en:\n",
    "    - GBR {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 300, 'subsample': 0.7} -> 0.4038856\n",
    "    - Best PCorr - BLEURT -> 0.37235\n",
    "- en-fi:\n",
    "    - LightGBM {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 300, 'subsample': 0.7} -> 0.537458\n",
    "    - Best PCorr - BLEURT -> 0.52858\n",
    "- en-zh:\n",
    "    - LightGBM {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 300, 'subsample': 0.9}-> 0.445767\n",
    "    - Best PCorr - rouge_f1_w1\t-> \t0.436083\n",
    "- ru-en:\n",
    "    - LightGBM {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 800, 'subsample': 0.9} -> 0.411640\n",
    "    - Best PCorr - BLEURT-> 0.389512\n",
    "- zh-en:\n",
    "    - LightGBM {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.9} -> 0.390421\n",
    "    - Best PCorr - BLEURT-> 0.343382\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSupervisedData(train_data, test_data):\n",
    "    scaler = StandardScaler()\n",
    "    # scaling train data\n",
    "    train_scores = train_data.loc[:,'bleu_w1':]\n",
    "    train_scores = pd.DataFrame(scaler.fit_transform(train_scores),columns=train_scores.columns, index = train_scores.index)\n",
    "    train_scores.fillna(0,inplace=True)\n",
    "    # transforming test data\n",
    "    test_scores = test_data.loc[:,'bleu_w1':]\n",
    "    test_scores = pd.DataFrame(scaler.transform(test_scores),columns=test_scores.columns, index = test_scores.index)\n",
    "    test_scores.fillna(0,inplace=True)\n",
    "    return train_scores, test_scores\n",
    "def pipeline(corpus, model_dict):\n",
    "    train_path = \"corpus\\\\\" \n",
    "    test_path = \"testset\\\\\"   \n",
    "    print(f\"------------ Corpus [{corpus}] -------------\")\n",
    "    print(\"**Preprocessing and scoring train data...\")\n",
    "    train_data = main(train_path + corpus + \"\\\\scores.csv\")\n",
    "    print(\"**Preprocessing and scoring test data...\")\n",
    "    test_data = main(test_path + corpus + \"\\\\scores.csv\")\n",
    "    train_scores, test_scores = generateSupervisedData(train_data, test_data)\n",
    "    model = model_dict[corpus]\n",
    "    print(\"**Training regression model\")\n",
    "    model.fit(train_scores, train_data['z-score'])\n",
    "    prediction = pd.Series(model.predict(test_scores),name = \"metric\", index = test_scores.index)\n",
    "    test_df = getCSV(test_path + corpus + \"\\\\scores.csv\")\n",
    "    final_df = pd.concat([test_df, prediction], axis = 1)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Corpus [ru-en] -------------\n",
      "**Preprocessing and scoring train data...\n",
      "Detected ru-en...Skip preprocessing...\n",
      "BLEU calculated...\n",
      "ROUGE 1-gram calculated...\n",
      "ROUGE 2-gram calculated...\n",
      "INFO:tensorflow:Reading checkpoint bleurt\\test_checkpoint.\n",
      "INFO:tensorflow:Config file found, reading.\n",
      "INFO:tensorflow:Will load checkpoint dbleurt_tiny\n",
      "INFO:tensorflow:Loads full paths and checks that files exists.\n",
      "INFO:tensorflow:... name:dbleurt_tiny\n",
      "INFO:tensorflow:... vocab_file:vocab.txt\n",
      "INFO:tensorflow:... bert_config_file:bert_config.json\n",
      "INFO:tensorflow:... do_lower_case:True\n",
      "INFO:tensorflow:... max_seq_length:512\n",
      "INFO:tensorflow:Creating BLEURT scorer.\n",
      "INFO:tensorflow:Creating WordPiece tokenizer.\n",
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n",
      "INFO:tensorflow:Creating Eager Mode predictor.\n",
      "INFO:tensorflow:Loading model.\n",
      "INFO:tensorflow:BLEURT initialized.\n",
      "BLEURT calculating...\n",
      "**Preprocessing and scoring test data...\n",
      "Detected ru-en...Skip preprocessing...\n",
      "BLEU calculated...\n",
      "ROUGE 1-gram calculated...\n",
      "ROUGE 2-gram calculated...\n",
      "INFO:tensorflow:Reading checkpoint bleurt\\test_checkpoint.\n",
      "INFO:tensorflow:Config file found, reading.\n",
      "INFO:tensorflow:Will load checkpoint dbleurt_tiny\n",
      "INFO:tensorflow:Loads full paths and checks that files exists.\n",
      "INFO:tensorflow:... name:dbleurt_tiny\n",
      "INFO:tensorflow:... vocab_file:vocab.txt\n",
      "INFO:tensorflow:... bert_config_file:bert_config.json\n",
      "INFO:tensorflow:... do_lower_case:True\n",
      "INFO:tensorflow:... max_seq_length:512\n",
      "INFO:tensorflow:Creating BLEURT scorer.\n",
      "INFO:tensorflow:Creating WordPiece tokenizer.\n",
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n",
      "INFO:tensorflow:Creating Eager Mode predictor.\n",
      "INFO:tensorflow:Loading model.\n",
      "INFO:tensorflow:BLEURT initialized.\n",
      "BLEURT calculating...\n",
      "**Training regression model\n",
      "------------ Finished [ru-en] -------------\n",
      "------------ Corpus [zh-en] -------------\n",
      "**Preprocessing and scoring train data...\n",
      "Preprocessing...\n",
      "BLEU calculated...\n",
      "ROUGE 1-gram calculated...\n",
      "ROUGE 2-gram calculated...\n",
      "INFO:tensorflow:Reading checkpoint bleurt\\test_checkpoint.\n",
      "INFO:tensorflow:Config file found, reading.\n",
      "INFO:tensorflow:Will load checkpoint dbleurt_tiny\n",
      "INFO:tensorflow:Loads full paths and checks that files exists.\n",
      "INFO:tensorflow:... name:dbleurt_tiny\n",
      "INFO:tensorflow:... vocab_file:vocab.txt\n",
      "INFO:tensorflow:... bert_config_file:bert_config.json\n",
      "INFO:tensorflow:... do_lower_case:True\n",
      "INFO:tensorflow:... max_seq_length:512\n",
      "INFO:tensorflow:Creating BLEURT scorer.\n",
      "INFO:tensorflow:Creating WordPiece tokenizer.\n",
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n",
      "INFO:tensorflow:Creating Eager Mode predictor.\n",
      "INFO:tensorflow:Loading model.\n",
      "INFO:tensorflow:BLEURT initialized.\n",
      "BLEURT calculating...\n",
      "**Preprocessing and scoring test data...\n",
      "Preprocessing...\n",
      "BLEU calculated...\n",
      "ROUGE 1-gram calculated...\n",
      "ROUGE 2-gram calculated...\n",
      "INFO:tensorflow:Reading checkpoint bleurt\\test_checkpoint.\n",
      "INFO:tensorflow:Config file found, reading.\n",
      "INFO:tensorflow:Will load checkpoint dbleurt_tiny\n",
      "INFO:tensorflow:Loads full paths and checks that files exists.\n",
      "INFO:tensorflow:... name:dbleurt_tiny\n",
      "INFO:tensorflow:... vocab_file:vocab.txt\n",
      "INFO:tensorflow:... bert_config_file:bert_config.json\n",
      "INFO:tensorflow:... do_lower_case:True\n",
      "INFO:tensorflow:... max_seq_length:512\n",
      "INFO:tensorflow:Creating BLEURT scorer.\n",
      "INFO:tensorflow:Creating WordPiece tokenizer.\n",
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n",
      "INFO:tensorflow:Creating Eager Mode predictor.\n",
      "INFO:tensorflow:Loading model.\n",
      "INFO:tensorflow:BLEURT initialized.\n",
      "BLEURT calculating...\n",
      "**Training regression model\n",
      "------------ Finished [zh-en] -------------\n"
     ]
    }
   ],
   "source": [
    "model_dict = {\n",
    "    'cs-en':ensemble.GradientBoostingRegressor(learning_rate= 0.03, max_depth= 4, n_estimators= 200, subsample=0.8,random_state = 2),\n",
    "    'de-en':ensemble.GradientBoostingRegressor(learning_rate= 0.01, max_depth= 4, n_estimators= 300, subsample=0.7,random_state = 2),\n",
    "    'en-fi':lgb.LGBMRegressor(boosting_type='gbdt', learning_rate= 0.01, max_depth= 4, n_estimators= 300, subsample= 0.7,metric= 'rmse', random_state=2),\n",
    "    'en-zh':lgb.LGBMRegressor(boosting_type='gbdt', learning_rate= 0.01, max_depth= 3, n_estimators= 300, subsample= 0.9,metric= 'rmse', random_state=2),\n",
    "    'ru-en':lgb.LGBMRegressor(boosting_type='gbdt', learning_rate= 0.01, max_depth= 3, n_estimators= 800, subsample= 0.9,metric= 'rmse', random_state=2),\n",
    "    'zh-en':lgb.LGBMRegressor(boosting_type='gbdt', learning_rate= 0.05, max_depth= 3, n_estimators= 200, subsample= 0.9,metric= 'rmse', random_state=2)\n",
    "}\n",
    "corpus_list_0=['cs-en','de-en','en-fi','en-zh']\n",
    "corpus_list = ['ru-en','zh-en']\n",
    "for corpus in corpus_list:\n",
    "    predicted_df = pipeline(corpus, model_dict)\n",
    "    predicted_df.to_csv(\"prediction\\\\\" + corpus +\"\\\\scores.csv\",index=False)\n",
    "    print(f\"------------ Finished [{corpus}] -------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "f19adefd66748b3bd8823cd4c118b1cd30b9c160894c8c80efd6730bfc1ce2e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
